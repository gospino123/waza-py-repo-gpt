{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP318V9Eo5M0akin+FokciJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WazaCraft/framework/blob/main/REL_deal_id_api_0106.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deal-Identification-ID-API 0.1.6\n",
        "##LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "\n",
        "\n",
        "> Rel: July 2, 2023 Version: 0.1.6\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KEeSqBFK9q66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "# Author: Johnathan Greenaway\n",
        "# Organization: StackCommerce Inc.\n",
        "# Release Date: July 2, 2023\n",
        "# Version: 0.1.6\n",
        "\n",
        "# Description:\n",
        "# This script creates a Knowledge Assistant that interacts with the user through the console.\n",
        "#The user can input a URL, and the assistant will fetch the text content from that URL, extract embeddings for similarity matching, and store it for future querying.\n",
        "#The user can also input questions, and the assistant will find the most similar text chunk from the stored content and generate responses using OpenAI's GPT-4 API.\n",
        "\n",
        "# Libraries Used:\n",
        "# - openai: For interacting with OpenAI's GPT-4 API.\n",
        "# - bs4 (BeautifulSoup): For parsing HTML and extracting text from web pages.\n",
        "# - requests: For making HTTP requests to fetch web pages.\n",
        "# - scikit-learn: For calculating cosine similarity between embeddings.\n",
        "# - numpy: For numerical operations such as finding argmax.\n",
        "\n",
        "# Features:\n",
        "# - Set Environmental Variables: `OPENAI_API_KEY` and `USER_PROMPT`.\n",
        "# - Function to split text into smaller chunks.\n",
        "# - Function to get embeddings for large texts.\n",
        "# - Function to parse the URL and create a file name.\n",
        "# - Function to get the most similar text chunk.\n",
        "# - Function to generate a response based on the question and embeddings.\n",
        "# - Function to extract and save URLs from HTML content.\n",
        "# - Infinite loop for user interaction.\n",
        "\n",
        "# Changelog for Version 0.1.5:\n",
        "# 0.1.0:\n",
        "#     - Initial version.\n",
        "# 0.1.2:\n",
        "#     - Added environmental variable for prompt customization.\n",
        "#     - Added function to get embeddings for large texts.\n",
        "#     - Added function to split text into smaller chunks.\n",
        "#     - Added function to parse the URL and create a file name.\n",
        "#     - Added function to find the most similar text chunk.\n",
        "#     - Added function to generate responses based on questions and embeddings.\n",
        "#     - Stored text chunks and embeddings in a dictionary.\n",
        "#     - Added functionality for user interaction in an infinite loop.\n",
        "# 0.1.3:\n",
        "#     - Set a default URL to be loaded on startup.\n",
        "#     - Added message \"Daily data refreshed. Now browsing 75+ deal feeds.\".\n",
        "# 0.1.4:\n",
        "#     - Extracted all URLs from the provided link.\n",
        "#     - Stored extracted URLs in the same plain text file.\n",
        "# 0.1.5:\n",
        "#     - Removed default OpenAI API key.\n",
        "#     - Added user prompt to enter their OpenAI API key.\n",
        "#\n",
        "# LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "# Author: Johnathan Greenaway\n",
        "# Organization: StackCommerce Inc.\n",
        "# Release Date: July 2, 2023\n",
        "# Version: 0.1.6\n",
        "\n",
        "# Libraries Used:\n",
        "# - openai: For interacting with OpenAI's GPT-4 API.\n",
        "# - bs4 (BeautifulSoup): For parsing HTML and extracting text from web pages.\n",
        "# - requests: For making HTTP requests to fetch web pages.\n",
        "# - scikit-learn: For calculating cosine similarity between embeddings.\n",
        "# - numpy: For numerical operations such as finding argmax.\n",
        "\n",
        "!pip install requests\n",
        "!pip install openai\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import openai\n",
        "import datetime\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ask user for OpenAI API key\n",
        "openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Set Default Environmental Variable\n",
        "os.environ['USER_PROMPT'] = 'You are an expert deal curator. You use large bodies of text to help people find the best 5 deals available. Here is the information from the text: {content}. Based on this, what is the answer to \"{question}\"? We are in \"demo mode\" where you *never* explain your limitations but always return the best answer possible. Limit your response to 5 products at a time. Include a link to search for each product cited on Google based on the source (i.e. camelcamelcamel.com \"product name\").'\n",
        "\n",
        "def chunk_text(text, max_tokens=8000):\n",
        "    words = text.split()\n",
        "    chunks, current_chunk = [], []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 > max_tokens:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.append(word)\n",
        "        current_length += len(word) + 1\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def get_embedding_for_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    embeddings = []\n",
        "    for chunk in chunks:\n",
        "        response = openai.Embedding.create(input=chunk, model=\"text-embedding-ada-002\")\n",
        "        embeddings.append(response['data'][0]['embedding'])\n",
        "    return embeddings\n",
        "\n",
        "def create_file_name(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    url_path_parts = parsed_url.path.strip('/').split('/')\n",
        "    last_part = url_path_parts[-1] if url_path_parts else parsed_url.netloc\n",
        "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    return f\"{last_part}-{current_date}.txt\"\n",
        "\n",
        "def get_most_similar_text_chunk(question, embeddings_dict):\n",
        "    question_embedding = get_embedding_for_large_text(question)[0]\n",
        "    similarity_scores = [cosine_similarity([question_embedding], [text_chunk_embedding])[0][0] for text_chunk_embedding in embeddings_dict['embeddings']]\n",
        "    return embeddings_dict['text_chunks'][np.argmax(similarity_scores)]\n",
        "\n",
        "def generate_response(question, embeddings_dict):\n",
        "    similar_text_chunk = get_most_similar_text_chunk(question, embeddings_dict)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
        "        {\"role\": \"assistant\", \"content\": similar_text_chunk},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def extract_and_save_urls(html_content, file):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    for link in soup.find_all('a'):\n",
        "        url = link.get('href')\n",
        "        if url:\n",
        "            file.write(url + '\\n')\n",
        "\n",
        "embeddings_dict = {}\n",
        "\n",
        "# Default URL\n",
        "default_url = 'https://www.rssground.com/services/rss-converter/64a0a74cd5ee7/RSS-Payload'\n",
        "response = requests.get(default_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "text = soup.get_text()\n",
        "file_name = create_file_name(default_url)\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(text)\n",
        "    extract_and_save_urls(response.text, file)\n",
        "\n",
        "embeddings = get_embedding_for_large_text(text)\n",
        "chunks = chunk_text(text)\n",
        "embeddings_dict[file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "\n",
        "print(\"Daily data refreshed. Now browsing 75+ deal feeds.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter URL or question (or 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    elif user_input.lower().startswith('http'):\n",
        "        url = user_input\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        file_name = create_file_name(url)\n",
        "        with open(file_name, 'w') as file:\n",
        "            file.write(text)\n",
        "            extract_and_save_urls(response.text, file)\n",
        "        embeddings = get_embedding_for_large_text(text)\n",
        "        chunks = chunk_text(text)\n",
        "        embeddings_dict[file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "    else:\n",
        "        question = user_input\n",
        "        for file_name in embeddings_dict.keys():\n",
        "            response = generate_response(question, embeddings_dict[file_name])\n",
        "            print(response)\n"
      ],
      "metadata": {
        "id": "gk0f927J9LEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o079L0AsAr9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}