{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WazaCraft/framework/blob/main/REL_deal_id_api_0107.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deal-Identification-ID-API 0.1.7\n",
        "##LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "\n",
        "\n",
        "> Rel: July 2, 2023 Version: 0.1.7\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KEeSqBFK9q66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "# Author: Johnathan Greenaway\n",
        "# Organization: StackCommerce Inc.\n",
        "# Release Date: July 2, 2023\n",
        "# Version: 0.1.7\n",
        "#0.1.7.5\n",
        "#Fixed embedding naming\n",
        "#0.1.7.4\n",
        "#Reintroduced Pickle\n",
        "#0.1.7.3\n",
        "#Deal-Identification-ID-API 0.1.7\n",
        "#LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "#Added commands to enable / disable Flask server for REST API\n",
        "#Added reconfig server port path\n",
        "#0.1.7.2\n",
        "#Added Flask\n",
        "\n",
        "#To do: add vector selection\n",
        "!pip install openai\n",
        "!pip install bs4\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "!pip install Flask"
      ],
      "metadata": {
        "id": "qdSUyifVPsqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-Hb5p_a0vzUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import requests\n",
        "import openai\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pickle\n",
        "import socket\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
        "openai.api_key = openai_api_key\n",
        "os.environ['USER_PROMPT'] = 'You are an expert deal curator. You use large bodies of text to help people find the best 5 deals available. Here is the information from the text: {content}. Based on this, what is the answer to \"{question}\"? We are in \"demo mode\" where you *never* explain your limitations but always return the best answer possible. Limit your response to 5 products at a time. Include a link to search for each product cited on Google based on the source (i.e. camelcamelcamel.com \"product name\").'\n",
        "def chunk_text(text, max_tokens=8000):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 > max_tokens:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.append(word)\n",
        "        current_length += len(word) + 1\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def get_embedding_for_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    embeddings = []\n",
        "    for chunk in chunks:\n",
        "        response = openai.Embedding.create(input=chunk, model=\"text-embedding-ada-002\")\n",
        "        embedding = response['data'][0]['embedding']\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings\n",
        "\n",
        "def create_file_name(url, extension='txt'):\n",
        "    parsed_url = urlparse(url)\n",
        "    url_path_parts = parsed_url.path.strip('/').split('/')\n",
        "    last_part = url_path_parts[-1] if url_path_parts else parsed_url.netloc\n",
        "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    return f\"{last_part}-{current_date}.{extension}\"\n",
        "\n",
        "def get_most_similar_text_chunk(question, embeddings_dict):\n",
        "    question_embedding = get_embedding_for_large_text(question)[0]\n",
        "    similarity_scores = []\n",
        "    for text_chunk_embedding in embeddings_dict['embeddings']:\n",
        "        similarity_scores.append(cosine_similarity([question_embedding], [text_chunk_embedding])[0][0])\n",
        "    most_similar_index = np.argmax(similarity_scores)\n",
        "    return embeddings_dict['text_chunks'][most_similar_index]\n",
        "\n",
        "def generate_response(question, embeddings_dict):\n",
        "    similar_text_chunk = get_most_similar_text_chunk(question, embeddings_dict)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
        "        {\"role\": \"assistant\", \"content\": similar_text_chunk},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
        "        assistant_reply = response['choices'][0]['message']['content']\n",
        "        return assistant_reply\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def extract_and_save_urls(html_content, file):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    for link in soup.find_all('a'):\n",
        "        url = link.get('href')\n",
        "        if url:\n",
        "            file.write(url + '\\n')\n",
        "\n",
        "def save_embeddings_to_file(embeddings_dict, file_name):\n",
        "    with open(file_name, 'wb') as file:\n",
        "        pickle.dump(embeddings_dict, file)\n",
        "\n",
        "def load_embeddings_from_file(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "embeddings_dict = {}\n",
        "\n",
        "url = 'https://www.rssground.com/services/rss-converter/64a0a74cd5ee7/RSS-Payload'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "file_name = create_file_name(url)\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(text)\n",
        "    extract_and_save_urls(text, file)\n",
        "\n",
        "embeddings = get_embedding_for_large_text(text)\n",
        "chunks = chunk_text(text)\n",
        "embeddings_file_name = create_file_name(url, extension='pkl')\n",
        "embeddings_dict[embeddings_file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "save_embeddings_to_file(embeddings_dict, embeddings_file_name)\n",
        "\n",
        "print(\"Daily data refreshed. Now browsing 75+ deal feeds.\")\n",
        "\n",
        "@app.route('/ask', methods=['GET'])\n",
        "def ask_question():\n",
        "    question = request.args.get('question')\n",
        "    if question:\n",
        "        responses = []\n",
        "        for embeddings_file_name in embeddings_dict.keys():\n",
        "            response = generate_response(question, embeddings_dict[embeddings_file_name])\n",
        "            responses.append(response)\n",
        "        return jsonify(responses)\n",
        "    return jsonify({\"error\": \"No question provided\"})\n",
        "\n",
        "def run_web_api(port):\n",
        "    app.run(port=port)\n",
        "\n",
        "def is_port_in_use(port):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        return s.connect_ex(('localhost', port)) == 0\n",
        "\n",
        "api_thread = None\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter URL or question or 'deal-id up' or 'deal-id down' (or 'exit' to quit): \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    elif user_input.lower() == 'deal-id up':\n",
        "        if api_thread is None or not api_thread.is_alive():\n",
        "            port = 5000\n",
        "            while is_port_in_use(port):\n",
        "                port = int(input(f\"Port {port} is in use. Please enter a different port: \"))\n",
        "            api_thread = threading.Thread(target=run_web_api, args=(port,))\n",
        "            api_thread.daemon = True\n",
        "            api_thread.start()\n",
        "        else:\n",
        "            print(\"Server is already running\")\n",
        "\n",
        "    elif user_input.lower() == 'deal-id down':\n",
        "        if api_thread and api_thread.is_alive():\n",
        "            print(\"Stopping the server.\")\n",
        "            requests.post(f'http://localhost:{port}/shutdown')\n",
        "            api_thread.join()\n",
        "        else:\n",
        "            print(\"Server is not running\")\n",
        "\n",
        "    elif user_input.lower().startswith('http'):\n",
        "        url = user_input\n",
        "        response = requests.get(url)\n",
        "        text = response.text\n",
        "        file_name = create_file_name(url)\n",
        "\n",
        "        with open(file_name, 'w') as file:\n",
        "            file.write(text)\n",
        "            extract_and_save_urls(text, file)\n",
        "\n",
        "        embeddings = get_embedding_for_large_text(text)\n",
        "        chunks = chunk_text(text)\n",
        "        embeddings_file_name = create_file_name(url, extension='pkl')\n",
        "        embeddings_dict[embeddings_file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "        save_embeddings_to_file(embeddings_dict, embeddings_file_name)\n",
        "\n",
        "    else:\n",
        "        question = user_input\n",
        "        for embeddings_file_name in embeddings_dict.keys():\n",
        "            response = generate_response(question, embeddings_dict[embeddings_file_name])\n",
        "            print(response)\n"
      ],
      "metadata": {
        "id": "5lSvD69RndsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o079L0AsAr9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}