{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ9ktaeTUKiVJINbyV8GWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WazaCraft/framework/blob/main/REL_deal_id_api_0105and0106.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deal-Identification-ID-API Build 0105\n",
        "##LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "\n",
        "\n",
        "> Rel: July 2, 2023 Version: 0.1.6\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KEeSqBFK9q66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "# Author: Johnathan Greenaway\n",
        "# Organization: StackCommerce Inc.\n",
        "# Release Date: July 2, 2023\n",
        "# Version: 0.1.6\n",
        "\n",
        "# Description:\n",
        "# This script creates a Knowledge Assistant that interacts with the user through the console.\n",
        "#The user can input a URL, and the assistant will fetch the text content from that URL, extract embeddings for similarity matching, and store it for future querying.\n",
        "#The user can also input questions, and the assistant will find the most similar text chunk from the stored content and generate responses using OpenAI's GPT-4 API.\n",
        "\n",
        "# Libraries Used:\n",
        "# - openai: For interacting with OpenAI's GPT-4 API.\n",
        "# - bs4 (BeautifulSoup): For parsing HTML and extracting text from web pages.\n",
        "# - requests: For making HTTP requests to fetch web pages.\n",
        "# - scikit-learn: For calculating cosine similarity between embeddings.\n",
        "# - numpy: For numerical operations such as finding argmax.\n",
        "\n",
        "# Features:\n",
        "# - Set Environmental Variables: `OPENAI_API_KEY` and `USER_PROMPT`.\n",
        "# - Function to split text into smaller chunks.\n",
        "# - Function to get embeddings for large texts.\n",
        "# - Function to parse the URL and create a file name.\n",
        "# - Function to get the most similar text chunk.\n",
        "# - Function to generate a response based on the question and embeddings.\n",
        "# - Function to extract and save URLs from HTML content.\n",
        "# - Infinite loop for user interaction.\n",
        "\n",
        "# Changelog for Version 0.1.5:\n",
        "# 0.1.0:\n",
        "#     - Initial version.\n",
        "# 0.1.2:\n",
        "#     - Added environmental variable for prompt customization.\n",
        "#     - Added function to get embeddings for large texts.\n",
        "#     - Added function to split text into smaller chunks.\n",
        "#     - Added function to parse the URL and create a file name.\n",
        "#     - Added function to find the most similar text chunk.\n",
        "#     - Added function to generate responses based on questions and embeddings.\n",
        "#     - Stored text chunks and embeddings in a dictionary.\n",
        "#     - Added functionality for user interaction in an infinite loop.\n",
        "# 0.1.3:\n",
        "#     - Set a default URL to be loaded on startup.\n",
        "#     - Added message \"Daily data refreshed. Now browsing 75+ deal feeds.\".\n",
        "# 0.1.4:\n",
        "#     - Extracted all URLs from the provided link.\n",
        "#     - Stored extracted URLs in the same plain text file.\n",
        "# 0.1.5:\n",
        "#     - Removed default OpenAI API key.\n",
        "#     - Added user prompt to enter their OpenAI API key.\n",
        "#\n",
        "# LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "# Author: Johnathan Greenaway\n",
        "# Organization: StackCommerce Inc.\n",
        "# Release Date: July 2, 2023\n",
        "# Version: 0.1.6\n",
        "\n",
        "# Libraries Used:\n",
        "# - openai: For interacting with OpenAI's GPT-4 API.\n",
        "# - bs4 (BeautifulSoup): For parsing HTML and extracting text from web pages.\n",
        "# - requests: For making HTTP requests to fetch web pages.\n",
        "# - scikit-learn: For calculating cosine similarity between embeddings.\n",
        "# - numpy: For numerical operations such as finding argmax.\n",
        "\n",
        "!pip install requests\n",
        "!pip install openai\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import openai\n",
        "import datetime\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ask user for OpenAI API key\n",
        "openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Set Default Environmental Variable\n",
        "os.environ['USER_PROMPT'] = 'You are an expert deal curator. You use large bodies of text to help people find the best 5 deals available. Here is the information from the text: {content}. Based on this, what is the answer to \"{question}\"? We are in \"demo mode\" where you *never* explain your limitations but always return the best answer possible. Limit your response to 5 products at a time. Include a link to search for each product cited on Google based on the source (i.e. camelcamelcamel.com \"product name\").'\n",
        "\n",
        "def chunk_text(text, max_tokens=8000):\n",
        "    words = text.split()\n",
        "    chunks, current_chunk = [], []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 > max_tokens:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.append(word)\n",
        "        current_length += len(word) + 1\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def get_embedding_for_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    embeddings = []\n",
        "    for chunk in chunks:\n",
        "        response = openai.Embedding.create(input=chunk, model=\"text-embedding-ada-002\")\n",
        "        embeddings.append(response['data'][0]['embedding'])\n",
        "    return embeddings\n",
        "\n",
        "def create_file_name(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    url_path_parts = parsed_url.path.strip('/').split('/')\n",
        "    last_part = url_path_parts[-1] if url_path_parts else parsed_url.netloc\n",
        "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    return f\"{last_part}-{current_date}.txt\"\n",
        "\n",
        "def get_most_similar_text_chunk(question, embeddings_dict):\n",
        "    question_embedding = get_embedding_for_large_text(question)[0]\n",
        "    similarity_scores = [cosine_similarity([question_embedding], [text_chunk_embedding])[0][0] for text_chunk_embedding in embeddings_dict['embeddings']]\n",
        "    return embeddings_dict['text_chunks'][np.argmax(similarity_scores)]\n",
        "\n",
        "def generate_response(question, embeddings_dict):\n",
        "    similar_text_chunk = get_most_similar_text_chunk(question, embeddings_dict)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
        "        {\"role\": \"assistant\", \"content\": similar_text_chunk},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def extract_and_save_urls(html_content, file):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    for link in soup.find_all('a'):\n",
        "        url = link.get('href')\n",
        "        if url:\n",
        "            file.write(url + '\\n')\n",
        "\n",
        "embeddings_dict = {}\n",
        "\n",
        "# Default URL\n",
        "default_url = 'https://www.rssground.com/services/rss-converter/64a0a74cd5ee7/RSS-Payload'\n",
        "response = requests.get(default_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "text = soup.get_text()\n",
        "file_name = create_file_name(default_url)\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(text)\n",
        "    extract_and_save_urls(response.text, file)\n",
        "\n",
        "embeddings = get_embedding_for_large_text(text)\n",
        "chunks = chunk_text(text)\n",
        "embeddings_dict[file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "\n",
        "print(\"Daily data refreshed. Now browsing 75+ deal feeds.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter URL or question (or 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    elif user_input.lower().startswith('http'):\n",
        "        url = user_input\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        file_name = create_file_name(url)\n",
        "        with open(file_name, 'w') as file:\n",
        "            file.write(text)\n",
        "            extract_and_save_urls(response.text, file)\n",
        "        embeddings = get_embedding_for_large_text(text)\n",
        "        chunks = chunk_text(text)\n",
        "        embeddings_dict[file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "    else:\n",
        "        question = user_input\n",
        "        for file_name in embeddings_dict.keys():\n",
        "            response = generate_response(question, embeddings_dict[file_name])\n",
        "            print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gk0f927J9LEj",
        "outputId": "5a3982bb-170e-4ad7-ea0f-6db6623d0cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Please enter your OpenAI API key: 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 \n",
            "Daily data refreshed. Now browsing 75+ deal feeds.\n",
            "Enter URL or question (or 'exit' to quit): Give me pizza deals\n",
            "I'm sorry for any confusion, but as an AI, I don't have real-time capabilities to monitor or provide current deals or offers, including pizza deals. I would recommend checking directly on the websites of popular pizza outlets like Domino's, Pizza Hut, or Papa John's, as they frequently have special promotions and deals. You may also want to consider searching for coupon codes online or using apps that are specifically designed to provide food and restaurant coupons and deals.\n",
            "Enter URL or question (or 'exit' to quit): What are good deals for gamers today\n",
            "Here are some good deals for gamers available today:\n",
            "\n",
            "1. SAMSUNG 870 EVO SATA III SSD 1TB 2.5” Internal Solid State Drive - Current Price: $49.11, List Price: $59.99. This SSD can upgrade PC or Laptop memory which can provide a smoother gaming experience. [Buy on Amazon](https://www.amazon.com/dp/B08Q82TJV1)\n",
            "\n",
            "2. SanDisk 2TB Extreme Portable SSD - Up to 1050MB/s - USB-C, USB 3.2 Gen 2 - External Solid State Drive - Current Price: $119.99, List Price: $459.99. This external SSD can provide extra storage for your games. [Buy on Amazon](https://www.amazon.com/dp/B08GYKNCCP)\n",
            "\n",
            "3. SAMSUNG 980 PRO SSD 1TB PCIe 4.0 NVMe Gen 4 Gaming M.2 Internal Solid State Drive Memory Card - Current Price: $58.62, List Price: $79.99. This is a high-performance internal SSD for gaming needs. [Buy on Amazon](https://www.amazon.com/dp/B08QBJ2YMG)\n",
            "\n",
            "4. Nintendo Switch with Neon Blue and Neon Red Joy-Con - Current Price: $299.00, List Price: $299.99. This is a popular gaming console that can be played at home or on the go. [Buy on Amazon](https://www.amazon.com/dp/B01MUAGZ49)\n",
            "\n",
            "5. GEARWRENCH 56 Pc. 3/8\" Drive 6 Pt. 120XP Mechanics Tool Set - Current Price: $64.32, List Price: $231.12. Although not directly game-related, it's a handy tool set for setting up and maintaining your gaming rigs. [Buy on Amazon](https://www.amazon.com/dp/B00BTEXQS0)\n",
            "\n",
            "6. SUPER MARIO Nintendo Dungeon 2.5” Figure Multipack Diorama Set with Accessories - Current Price: $7.93, List Price: $19.99. This is a nice collectable for gamers who are Mario fans. [Buy on Amazon](https://www.amazon.com/dp/B07GBMKR3Q)\n",
            "\n",
            "Remember to always check before buying as prices can fluctuate.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fbf4f6aee078>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter URL or question (or 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deal-Identification-ID-API Build 0105"
      ],
      "metadata": {
        "id": "FxzZKzUV89VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "###Author: Johnathan Greenaway\n",
        "###Organization: StackCommerce Inc.\n",
        "####Release Date: July 2, 2023 Version: 0.1.5\n",
        "\n",
        "Description: This script creates a Knowledge Assistant that interacts with the user through the console. The user can input a URL, and the assistant will fetch the text content from that URL, extract embeddings for similarity matching, and store it for future querying. The user can also input questions, and the assistant will find the most similar text chunk from the stored content and generate responses using OpenAI's GPT-4 API.\n",
        "\n",
        "The script allows customization of the prompt wrapper through an environment variable, letting the user change how the assistant presents information in responses.\n",
        "\n",
        "**OPENAI_API_KEY**: The key associated with this script has a low-ish threshold. Use sparingly.\n",
        "\n",
        "**Usage**:\n",
        "Set the necessary environment variables.\n",
        "Run the script.\n",
        "Input a URL for the assistant to process.\n",
        "Input questions for the assistant to answer based on the processed content.\n",
        "Note: This script includes an infinite loop for user interaction. Type 'exit' to quit the script.\n",
        "\n",
        "Limitations: Sometimes chat-gpt will misinterpret the request and think you're asking the completions API to search the internet. If that happens, go a little broader. \"\"\""
      ],
      "metadata": {
        "id": "S26Ktcnk2hxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changelog for Version 0.1.5\n",
        "# New Features:\n",
        "# 1. Environmental Variable for User Prompt Template: Added an environmental variable `USER_PROMPT` to allow users to\n",
        "#    set a custom prompt template. The assistant will use this prompt template while generating responses.\n",
        "# 2. Default URL Loading: The code now loads a default URL (https://www.rssground.com/services/rss-converter/64a0a74cd5ee7/RSS-Payload)\n",
        "#    and prints a message \"Daily data refreshed. Now browsing 75+ deal feeds.\" upon successful loading.\n",
        "# 3. Store Vector Embeddings: The script now stores the vector embeddings of the text data for later use.\n",
        "# 4. Extract All URLs: All URLs present in the HTML content are now extracted and stored in the same plain text file.\n",
        "# 5. Default promtp updated to route user to a Google search instead of direct link (avoids broken urls)\n",
        "\n",
        "# Initial Release Notes - Version 01a2r\n",
        "#\n",
        "# LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "# Author: Johnathan Greenaway\n",
        "# Organization: StackCommerce Inc.\n",
        "# Release Date: July 2, 2023\n",
        "# Version: 0.1.2\n",
        "\n",
        "# Description:\n",
        "# This script creates a Knowledge Assistant that interacts with the user through the console. The user can input a URL, and the assistant will fetch the text content from that URL, extract embeddings for similarity matching, and store it for future querying. The user can also input questions, and the assistant will find the most similar text chunk from the stored content and generate responses using OpenAI's GPT-4 API.\n",
        "\n",
        "# Libraries Used:\n",
        "# - openai: For interacting with OpenAI's GPT-4 API.\n",
        "# - bs4 (BeautifulSoup): For parsing HTML and extracting text from web pages.\n",
        "# - requests: For making HTTP requests to fetch web pages.\n",
        "# - scikit-learn: For calculating cosine similarity between embeddings.\n",
        "# - numpy: For numerical operations such as finding argmax.\n",
        "\n",
        "# Features:\n",
        "# - Set Environmental Variables: `OPENAI_API_KEY` and `USER_PROMPT`.\n",
        "# - Function to split text into smaller chunks.\n",
        "# - Function to get embeddings for large texts.\n",
        "# - Function to parse the URL and create a file name.\n",
        "# - Function to get the most similar text chunk.\n",
        "# - Function to generate a response based on the question and embeddings.\n",
        "# - Function to extract and save URLs from HTML content.\n",
        "# - Infinite loop for user interaction.\n",
        "\n",
        "!pip install requests\n",
        "!pip install openai\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import openai\n",
        "import datetime\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set Environmental Variables\n",
        "os.environ['OPENAI_API_KEY'] = 'YOUR-OPENAI-API-KEY-HERE'\n",
        "os.environ['USER_PROMPT'] = 'You are an expert deal curator. You use large bodies of text to help people find the best 5 deals available. Here is the information from the text: {content}. Based on this, what is the answer to \"{question}\"? Limit your response to 5 products at a time. Include a link to search for each product cited on Google based on the source (i.e. camelcamelcamel.com \"product name\").'\n",
        "# Set OpenAI API key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Function to split text into smaller chunks\n",
        "def chunk_text(text, max_tokens=8000):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 > max_tokens:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.append(word)\n",
        "        current_length += len(word) + 1\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to get embeddings for large texts\n",
        "def get_embedding_for_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    embeddings = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        response = openai.Embedding.create(input=chunk, model=\"text-embedding-ada-002\")\n",
        "        embedding = response['data'][0]['embedding']\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Function to parse the URL and create a file name\n",
        "def create_file_name(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    url_path_parts = parsed_url.path.strip('/').split('/')\n",
        "    last_part = url_path_parts[-1] if url_path_parts else parsed_url.netloc\n",
        "\n",
        "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    file_name = f\"{last_part}-{current_date}.txt\"\n",
        "    return file_name\n",
        "\n",
        "# Function to get the most similar text chunk\n",
        "def get_most_similar_text_chunk(question, embeddings_dict):\n",
        "    # Get embedding of the question\n",
        "    question_embedding = get_embedding_for_large_text(question)[0]\n",
        "\n",
        "    # Calculate similarity scores with all text chunks\n",
        "    similarity_scores = []\n",
        "    for text_chunk_embedding in embeddings_dict['embeddings']:\n",
        "        similarity_scores.append(cosine_similarity([question_embedding], [text_chunk_embedding])[0][0])\n",
        "\n",
        "    # Get the index of the most similar text chunk\n",
        "    most_similar_index = np.argmax(similarity_scores)\n",
        "\n",
        "    # Return the most similar text chunk\n",
        "    return embeddings_dict['text_chunks'][most_similar_index]\n",
        "\n",
        "# Function to generate a response based on the question and embeddings\n",
        "def generate_response(question, embeddings_dict):\n",
        "    # Get the most similar text chunk\n",
        "    similar_text_chunk = get_most_similar_text_chunk(question, embeddings_dict)\n",
        "\n",
        "    # Format user prompt\n",
        "    user_prompt = os.getenv('USER_PROMPT').format(content=similar_text_chunk, question=question)\n",
        "\n",
        "    # Start a conversation with a system message (optional)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
        "        {\"role\": \"assistant\", \"content\": user_prompt},  # Pass the formatted user prompt to the assistant\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    # Generate a response using the OpenAI ChatCompletion API\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages\n",
        "        )\n",
        "        # Extract the assistant's reply from the response\n",
        "        assistant_reply = response['choices'][0]['message']['content']\n",
        "        return assistant_reply\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "# Function to extract and save URLs from HTML content\n",
        "def extract_and_save_urls(html_content, file):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    urls = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "    for url in urls:\n",
        "        file.write(f'{url}\\n')\n",
        "    file.write('\\n') # Separate URLs from the text content with an empty line\n",
        "\n",
        "# Dictionary to store embeddings\n",
        "embeddings_dict = {}\n",
        "\n",
        "# Load default URL and store its content\n",
        "default_url = 'https://www.rssground.com/services/rss-converter/64a0a74cd5ee7/RSS-Payload'\n",
        "default_response = requests.get(default_url)\n",
        "default_soup = BeautifulSoup(default_response.text, 'html.parser')\n",
        "default_text = default_soup.get_text()\n",
        "default_file_name = create_file_name(default_url)\n",
        "with open(default_file_name, 'w') as file:\n",
        "    extract_and_save_urls(default_response.text, file) # Save URLs\n",
        "    file.write(default_text) # Save text content\n",
        "default_embeddings = get_embedding_for_large_text(default_text)\n",
        "default_chunks = chunk_text(default_text)\n",
        "embeddings_dict[default_file_name] = {'text_chunks': default_chunks, 'embeddings': default_embeddings}\n",
        "print(\"Daily data refreshed. Now browsing 75+ deal feeds.\")\n",
        "\n",
        "# Infinite loop for user interaction\n",
        "while True:\n",
        "    # Request user input\n",
        "    user_input = input(\"Enter URL or question (or 'exit' to quit): \")\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Check if input is URL\n",
        "    elif user_input.lower().startswith('http'):\n",
        "        url = user_input\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        file_name = create_file_name(url)\n",
        "\n",
        "        # Store the URLs and text in a file\n",
        "        with open(file_name, 'w') as file:\n",
        "            extract_and_save_urls(response.text, file) # Save URLs\n",
        "            file.write(text) # Save text content\n",
        "\n",
        "        # Get embeddings for the text\n",
        "        embeddings = get_embedding_for_large_text(text)\n",
        "        chunks = chunk_text(text)\n",
        "\n",
        "        # Store the text chunks and embeddings in the dictionary\n",
        "        embeddings_dict[file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "\n",
        "    # If input is not URL, consider it a question\n",
        "    else:\n",
        "        question = user_input\n",
        "        for file_name in embeddings_dict.keys():\n",
        "            response = generate_response(question, embeddings_dict[file_name])\n",
        "            print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l3xZLHDIyPTj",
        "outputId": "8f66ad2b-1ddb-4199-94c0-73d3f68ac058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main_parser.py\", line 10, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/cmdoptions.py\", line 603, in <module>\n",
            "    help=dedent(\n",
            "  File \"/usr/lib/python3.10/textwrap.py\", line 469, in dedent\n",
            "    text = re.sub(r'(?m)^' + margin, '', text)\n",
            "  File \"/usr/lib/python3.10/re.py\", line 209, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "  File \"/usr/lib/python3.10/re.py\", line 303, in _compile\n",
            "    p = sre_compile.compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.10/sre_compile.py\", line 788, in compile\n",
            "    p = sre_parse.parse(p, flags)\n",
            "  File \"/usr/lib/python3.10/sre_parse.py\", line 947, in parse\n",
            "    source = Tokenizer(str)\n",
            "  File \"/usr/lib/python3.10/sre_parse.py\", line 225, in __init__\n",
            "    def __init__(self, string):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0b8bafb40d82>\u001b[0m in \u001b[0;36m<cell line: 165>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mextract_and_save_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Save URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Save text content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0mdefault_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_for_large_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0mdefault_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0membeddings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_file_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text_chunks'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_embeddings\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0b8bafb40d82>\u001b[0m in \u001b[0;36mget_embedding_for_large_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-ada-002\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/embedding.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# If a user specifies base64, we'll just return the encoded string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: YOUR-OPE************HERE. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o079L0AsAr9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}